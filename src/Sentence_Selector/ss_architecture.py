"""
ss_architecture.py
Sentence Retrieval ëª¨ë¸
Bi-encoder ë°©ì‹ìœ¼ë¡œ Claimê³¼ Sentence ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê´€ë ¨ ë¬¸ì¥ì„ ê²€ìƒ‰
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Optional, Union
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os
from safetensors.torch import load_file

class SentenceRetrievalModel(nn.Module):
    """
    Sentence Retrieval ëª¨ë¸ (Bi-encoder ë°©ì‹)
    Claimê³¼ Sentenceë¥¼ ê°ê° ì¸ì½”ë”©í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_path: str = "sentence-transformers/all-MiniLM-L6-v2", # [ìˆ˜ì •] model_name -> model_path ë³€ê²½
        device: Optional[str] = None
    ):
        """
        Args:
            model_path: ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ(.safetensors) ë˜ëŠ” HuggingFace ëª¨ë¸ëª…
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        super(SentenceRetrievalModel, self).__init__()
        
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ”„ [SS Model] Loading Sentence Retrieval Model from: {model_path}")
        
        # ---------------------------------------------------------------------
        # [ë¡œë”© ë¡œì§ ê°œì„ ] ë¡œì»¬ íŒŒì¼(.safetensors)ì¸ ê²½ìš° ì²˜ë¦¬
        # ---------------------------------------------------------------------
        # 1. ë‹¨ì¼ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì§€ì •ëœ ê²½ìš° (config.json ì—†ìŒ)
        if os.path.isfile(model_path):
            print(f"   âš ï¸ Detected local weight file. Initializing base architecture first.")
            # ê»ë°ê¸°(ì•„í‚¤í…ì²˜)ëŠ” ê¸°ë³¸ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            base_model_name = "sentence-transformers/all-MiniLM-L6-v2"
            self.encoder = SentenceTransformer(base_model_name, device=self.device)
            
            # ê°€ì¤‘ì¹˜ ë®ì–´ì”Œìš°ê¸°
            print(f"   ğŸ“‚ Loading weights from: {model_path}")
            if model_path.endswith(".safetensors"):
                state_dict = load_file(model_path)
            else:
                state_dict = torch.load(model_path, map_location=self.device)
            
            # 'encoder.' ì ‘ë‘ì‚¬ê°€ ìˆë‹¤ë©´ ì œê±°í•˜ê³  ë¡œë“œ
            new_state_dict = {}
            for k, v in state_dict.items():
                if k.startswith("encoder."):
                    new_state_dict[k.replace("encoder.", "")] = v
                else:
                    new_state_dict[k] = v
            
            self.encoder.load_state_dict(new_state_dict, strict=False)
            print("   âœ… Local weights loaded successfully.")
            
        # 2. ì¼ë°˜ì ì¸ ê²½ìš° (HuggingFace ì´ë¦„ ë˜ëŠ” í´ë” ê²½ë¡œ)
        else:
            self.encoder = SentenceTransformer(model_path, device=self.device)
            
        self.embedding_dim = self.encoder.get_sentence_embedding_dimension()
        print(f"   - Device: {self.device}")
        print(f"   - Embedding Dim: {self.embedding_dim}")
    
    def encode_claim(self, claims: List[str], requires_grad: bool = False) -> Union[np.ndarray, torch.Tensor]:
        if not requires_grad:
            self.encoder.eval()
            with torch.no_grad():
                return self.encoder.encode(claims, convert_to_numpy=True, show_progress_bar=False, batch_size=32)
        else:
            self.encoder.train()
            tokenizer = self.encoder.tokenizer
            model = self.encoder[0].auto_model
            
            encoded = tokenizer(claims, padding=True, truncation=True, max_length=512, return_tensors='pt')
            encoded = {k: v.to(self.device) for k, v in encoded.items()}
            
            model_output = model(**encoded)
            embeddings = model_output[0]
            input_mask_expanded = encoded['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()
            sum_embeddings = torch.sum(embeddings * input_mask_expanded, 1)
            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
            return sum_embeddings / sum_mask

    def encode_sentences(self, sentences: List[str], requires_grad: bool = False) -> Union[np.ndarray, torch.Tensor]:
        # ë¬¸ì¥ ì¸ì½”ë”©ì€ Claim ì¸ì½”ë”©ê³¼ ë™ì¼í•œ ë°©ì‹ ì‚¬ìš©
        return self.encode_claim(sentences, requires_grad)

    def retrieve(self, claim: str, candidate_sentences: List[str], top_k: int = 5) -> List[Tuple[int, float]]:
        if not candidate_sentences:
            return []
            
        # ì¸ì½”ë”©
        claim_emb = self.encode_claim([claim])[0]
        sent_embs = self.encode_sentences(candidate_sentences)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity([claim_emb], sent_embs)[0]
        
        # Top-K ì„ íƒ
        top_k = min(top_k, len(candidate_sentences))
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = [(int(idx), float(similarities[idx])) for idx in top_indices]
        return results