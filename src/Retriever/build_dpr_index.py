import os
import json
import torch
import numpy as np
import faiss
from tqdm import tqdm
from transformers import DPRContextEncoder, AutoTokenizer
from pyserini.search.lucene import LuceneSearcher

# =========================================================
# [ì„¤ì •] ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
# =========================================================
BASE_PATH = os.path.dirname(os.path.abspath(__file__))

# 1. í•™ìŠµëœ Context Encoder ê²½ë¡œ
CTX_ENCODER_PATH = os.path.join(BASE_PATH, "saved_dpr_models", "dpr_finetuned", "best_model", "ctx_encoder")

# 2. Lucene ì¸ë±ìŠ¤ ê²½ë¡œ
WIKI_INDEX_PATH = os.path.join(BASE_PATH, "hover_wiki_index")

# 3. ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ
OUTPUT_FAISS_PATH = os.path.join(BASE_PATH, "hover_dpr_index")

# [ì¤‘ìš”] ì†ë„ í–¥ìƒì„ ìœ„í•´ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¼ (VRAM 16GB ê¸°ì¤€ 128~256 ì¶”ì²œ)
# OOM(Out of Memory) ì—ëŸ¬ê°€ ë‚˜ë©´ 64ë¡œ ì¤„ì´ì„¸ìš”.
BATCH_SIZE = 1536
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# =========================================================

def build_index():
    print(f"ğŸš€ [Start] Building FAISS Index (Fast Mode - FP16)...")
    print(f"   - Context Encoder: {CTX_ENCODER_PATH}")
    print(f"   - Batch Size     : {BATCH_SIZE}")
    print(f"   - Device         : {DEVICE}")

    # 1. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ (FP16 ì ìš©)
    try:
        # .half()ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ FP16 ëª¨ë“œë¡œ ì „í™˜ (ì†ë„ 2ë°° í–¥ìƒ)
        ctx_encoder = DPRContextEncoder.from_pretrained(CTX_ENCODER_PATH).to(DEVICE).half()
        ctx_encoder.eval()
        
        tokenizer = AutoTokenizer.from_pretrained("facebook/dpr-ctx_encoder-multiset-base")
        print("âœ… Context Encoder loaded in FP16 mode.")
    except Exception as e:
        print(f"âŒ Failed to load Context Encoder: {e}")
        return

    # 2. Lucene Index ë¡œë“œ
    if not os.path.exists(WIKI_INDEX_PATH):
        print(f"âŒ Lucene index not found at {WIKI_INDEX_PATH}")
        return
    
    searcher = LuceneSearcher(WIKI_INDEX_PATH)
    num_docs = searcher.num_docs
    print(f"âœ… Lucene Index loaded. Total documents: {num_docs}")

    # 3. FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    d = 768
    index = faiss.IndexFlatIP(d) # Inner Product (Cosine Sim)

    os.makedirs(OUTPUT_FAISS_PATH, exist_ok=True)

    # 4. ì¸ì½”ë”© ë£¨í”„
    doc_ids = []
    batch_texts = []
    batch_ids = []

    print("ğŸ”„ Encoding documents...")
    
    # torch.no_grad()ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë°©ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
    with torch.no_grad():
        for i in tqdm(range(num_docs), desc="Indexing"):
            try:
                # Luceneì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                doc = searcher.doc(i)
                if doc is None: continue
                
                raw_json = json.loads(doc.raw())
                
                # ID ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                d_id = raw_json.get('id') or raw_json.get('_id') or str(raw_json.get('title'))
                title = raw_json.get('title', "")
                text = raw_json.get('text') or raw_json.get('contents') or ""
                
                # DPR ì…ë ¥ í¬ë§·: "Title [SEP] Text"
                full_text = f"{title} [SEP] {text}"
                
                batch_texts.append(full_text)
                batch_ids.append(d_id)

                # ë°°ì¹˜ê°€ ê½‰ ì°¼ì„ ë•Œ ì¸ì½”ë”© ìˆ˜í–‰
                if len(batch_texts) >= BATCH_SIZE:
                    # í† í¬ë‚˜ì´ì§•
                    inputs = tokenizer(
                        batch_texts, 
                        return_tensors="pt", 
                        padding=True, 
                        truncation=True, 
                        max_length=256
                    ).to(DEVICE)
                    
                    # [í•µì‹¬] FP16 ì—°ì‚° ìˆ˜í–‰
                    # autocastë¥¼ ì“°ê±°ë‚˜ ëª¨ë¸ì´ ì´ë¯¸ .half() ìƒíƒœì´ë¯€ë¡œ ë°”ë¡œ forward
                    outputs = ctx_encoder(**inputs)
                    
                    # FP16 ê²°ê³¼ë¥¼ ë‹¤ì‹œ FP32(float32)ë¡œ ë³€í™˜ (FAISSëŠ” float32 ì„ í˜¸)
                    embeddings = outputs.pooler_output.float().cpu().numpy()
                    
                    # FAISSì— ì¶”ê°€
                    index.add(embeddings)
                    doc_ids.extend(batch_ids)
                    
                    # ë¦¬ì…‹
                    batch_texts = []
                    batch_ids = []

            except Exception as e:
                continue

        # ë‚¨ì€ ìíˆ¬ë¦¬ ë°°ì¹˜ ì²˜ë¦¬
        if batch_texts:
            inputs = tokenizer(
                batch_texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=256
            ).to(DEVICE)
            
            outputs = ctx_encoder(**inputs)
            embeddings = outputs.pooler_output.float().cpu().numpy()
            index.add(embeddings)
            doc_ids.extend(batch_ids)

    print(f"âœ… Encoding finished. Total vectors: {index.ntotal}")

    # 5. ì €ì¥
    faiss_file = os.path.join(OUTPUT_FAISS_PATH, "index")
    faiss.write_index(index, faiss_file)
    print(f"ğŸ’¾ Saved FAISS index to {faiss_file}")

    docid_file = os.path.join(OUTPUT_FAISS_PATH, "docid")
    with open(docid_file, 'w', encoding='utf-8') as f:
        for did in doc_ids:
            f.write(f"{did}\n")
    print(f"ğŸ’¾ Saved DocID mapping to {docid_file}")

if __name__ == "__main__":
    # CUDA ìºì‹œ ë¹„ìš°ê¸° (ë©”ëª¨ë¦¬ í™•ë³´)
    torch.cuda.empty_cache()
    build_index()