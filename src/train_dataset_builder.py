import os
import json
import spacy
import torch
import random
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
from pyserini.search.lucene import LuceneSearcher
from spacy.symbols import ORTH

# [1] í™˜ê²½ ë° ë¦¬ì†ŒìŠ¤ ì„¤ì •
os.environ["JAVA_HOME"] = r"C:\Program Files\Java\jdk-25"
os.environ["PYTHONUTF8"] = "1"

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ìš°ì„ ìˆœìœ„ ì„¤ì •)
USE_GPU = torch.cuda.is_available()
NUM_CORES = 6  # CPU ëª¨ë“œ ì‹œ ì‚¬ìš©í•  ì½”ì–´ ì œí•œ

# ê¸€ë¡œë²Œ ë³€ìˆ˜
searcher = None
nlp = None

def load_nlp_model():
    """ìƒí™©ì— ë§ëŠ” spaCy ëª¨ë¸ ë¡œë“œ"""
    if USE_GPU:
        # GPU ëª¨ë“œ: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì—ì„œ GPU ê°€ì† í™œìš©
        spacy.require_gpu()
        print("ğŸš€ [GPU Mode] Prioritizing GPU acceleration for Transformer models.")
    else:
        # CPU ëª¨ë“œ: ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš© ì˜ˆì •
        print(f"ğŸš€ [CPU Mode] Falling back to CPU with {NUM_CORES} cores.")
        
    try:
        model = spacy.load("en_core_web_trf")
    except:
        model = spacy.load("en_core_web_sm")
    model.tokenizer.add_special_case("gonna", [{ORTH: "gonna"}])
    return model

def init_worker(index_path):
    """CPU ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ìš© ì´ˆê¸°í™” í•¨ìˆ˜"""
    global searcher, nlp
    searcher = LuceneSearcher(index_path)
    # CPU ëª¨ë“œì—ì„œëŠ” ê°œë³„ í”„ë¡œì„¸ìŠ¤ê°€ ëª¨ë¸ì„ ë¡œë“œí•¨
    if not USE_GPU:
        nlp = load_nlp_model()

def get_passage_text(title, current_searcher):
    """Lucene ì¸ë±ìŠ¤ì—ì„œ ë³¸ë¬¸ ì¡°íšŒ"""
    hits = current_searcher.search(f"title:\"{title}\"", k=1)
    if hits:
        doc = current_searcher.doc(hits[0].docid)
        return json.loads(doc.raw()).get('text') or json.loads(doc.raw()).get('contents') or ""
    return ""

def get_logical_path(gold_titles, current_searcher):
    """[ë¡œì§] ë¸Œë¦¿ì§€ ì—”í‹°í‹° ê¸°ë°˜ ë…¼ë¦¬ì  ê²½ë¡œ ì¬ì •ë ¬"""
    if len(gold_titles) <= 1: return gold_titles
    title_to_text = {t: get_passage_text(t, current_searcher).lower() for t in gold_titles}
    temp_titles = gold_titles.copy()
    for i in range(len(temp_titles)):
        for j in range(len(temp_titles)):
            if i == j: continue
            p, c = temp_titles[i], temp_titles[j]
            if c.lower() in title_to_text.get(p, ""):
                idx_p, idx_c = temp_titles.index(p), temp_titles.index(c)
                if idx_p > idx_c:
                    temp_titles[idx_p], temp_titles[idx_c] = temp_titles[idx_c], temp_titles[idx_p]
    return temp_titles

def mine_hard_negatives(query_text, positive_titles, current_searcher, k=1):
    """[ë¡œì§] ì¿¼ë¦¬ë³„ ë™ì  Hard Negative Mining"""
    hits = current_searcher.search(query_text, k=20) 
    hard_negs = []
    for hit in hits:
        if hit.docid in positive_titles: continue
        doc = current_searcher.doc(hit.docid)
        if doc:
            doc_json = json.loads(doc.raw())
            title = doc_json.get('title', hit.docid)
            if title not in positive_titles:
                hard_negs.append({
                    "title": title,
                    "text": doc_json.get('text') or doc_json.get('contents') or "",
                    "score": hit.score
                })
        if len(hard_negs) >= k: break
    return hard_negs

def process_item(item, current_nlp, current_searcher):
    """ë‹¨ì¼ ë°ì´í„° ê°€ê³µ í•µì‹¬ ë¡œì§"""
    if item['label'] != 'SUPPORTED': return []

    claim = item['claim']
    gold_titles = list(set([fact[0] for fact in item['supporting_facts']]))
    logical_titles = get_logical_path(gold_titles, current_searcher)
    
    # Gold ë¬¸ì„œ ë³¸ë¬¸ ì‚¬ì „ ë¡œë“œ (ë¬¸ì¥ ë¶„ì ˆ í¬í•¨)
    gold_texts = {t: [s.text for s in current_nlp(get_passage_text(t, current_searcher)).sents] for t in gold_titles}
    
    doc = current_nlp(claim)
    candidates = []
    seen = set()
    HIGH_PRIORITY = {"PERSON", "ORG", "GPE", "LOC", "FAC", "PRODUCT", "EVENT", "WORK_OF_ART"}
    
    for ent in doc.ents:
        if ent.label_ in HIGH_PRIORITY:
            candidates.append(ent)
            seen.add(ent.text)
    for token in doc:
        if token.pos_ in ["NOUN", "PROPN", "PRON"] and token.text not in seen and not token.is_stop:
            candidates.append(doc[token.i : token.i + 1])
            seen.add(token.text)

    entries = []
    for span in candidates:
        anchor = span.text
        matched = [t for t in gold_titles if anchor.lower() in t.lower() or t.lower() in anchor.lower()]
        if not matched: continue
        best_pos_title = matched[0]

        # [ë¡œì§] ìˆœì„œ ë³´ì¡´í˜• ë¹„ëŒ€ì¹­ ìŠ¬ë¼ì´ì‹±
        # $$Slicing\_Context = doc[\max(0, span.start - 2) : \min(len(doc), span.end + 6)]$$
        start_i, end_i = max(0, span.start - 2), min(len(doc), span.end + 6)
        slicing_context = doc[start_i:end_i].text
        
        target_idx = logical_titles.index(best_pos_title)
        path_str = " -> ".join(["Claim"] + logical_titles[:target_idx])
        query_text = f"{anchor} [SEP] {path_str} [CTX] {slicing_context}"
        
        hard_negs = mine_hard_negatives(query_text, gold_titles, current_searcher, k=1)

        entries.append({
            "question": query_text,
            "positive_ctxs": [{"title": best_pos_title, "text": " ".join(gold_texts[best_pos_title])}],
            "negative_ctxs": [],
            "hard_negative_ctxs": hard_negs
        })
    return entries

# CPUìš© ë§¤í•‘ í•¨ìˆ˜
def cpu_worker(item):
    return process_item(item, nlp, searcher)

def build_dataset(input_path, output_path, index_path):
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    final_results = []

    if USE_GPU:
        # [GPU ëª¨ë“œ] ë³‘ë ¬ ì²˜ë¦¬ ì—†ì´ ë‹¨ì¼ ë£¨í”„ì—ì„œ GPU ê°€ì† ì‚¬ìš©
        print("ğŸš€ Starting Dataset Generation with GPU...")
        nlp_model = load_nlp_model()
        local_searcher = LuceneSearcher(index_path)
        
        for item in tqdm(data, desc="Processing (GPU)"):
            final_results.extend(process_item(item, nlp_model, local_searcher))
    else:
        # [CPU ëª¨ë“œ] 6ê°œ ì½”ì–´ ì œí•œ ë³‘ë ¬ ì²˜ë¦¬
        print(f"ğŸš€ Starting Dataset Generation with CPU (Cores: {NUM_CORES})...")
        with Pool(processes=NUM_CORES, initializer=init_worker, initargs=(index_path,)) as pool:
            for result_list in tqdm(pool.imap_unordered(cpu_worker, data), total=len(data), desc="Processing (CPU)"):
                final_results.extend(result_list)

    print(f"âœ… Saving {len(final_results)} entries to {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)

if __name__ == "__main__":
    IN_FILE = "./data/hover/hover_dev_release_v1.1.json"
    OUT_FILE = "./data/dpr_train_data/dpr_dev.json"
    IDX_PATH = "./hover_wiki_index"
    
    os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)
    build_dataset(IN_FILE, OUT_FILE, IDX_PATH)